# AI

AI is obviously AI. Has been around forever. 
But nowadays, AI is used to make AI.
So it's proper AI. 
Well, still not really. Still not really.
But I guess it's getting there slowly.


## Difference between normally programmed AI and a self learning AI i.e. an AI AI

So is what we program in a day to day program, is that not AI?

Well, yes, but that is a programmed AI. It can take input, process it according to some hard coded logic rules
that we program in, and then produce some output. And it can give the impression of being a general AI, if the rules are working on a complex set of inputs, but still, it can only
handle the input and the situtations that we program the logic for.
This kind of AI is called a symbolic AI.

A general AI does not need updated rules and logic from the programmer, really. It works by learning from the input data itself.
THe more data it gets, the more practice data it has to train its own logic, and create real outputs.
The way it builds up this logic, i.e. self improves or trains, can be different.
This kind of AI is called a statistical AI.

The main reason why AI AI works better now is that we just have more computing power, and a lot more collected data in general, and thus are able to work on larger sets of data.

### Machine Learning
Basically a branch of the data driven / statistical AI fields, where emphasis is on the system learning and adapting algorithms to match inputs to outputs, thus training itself without a need for human logic intervention or programming.

#### Deap Learning
Deep Learning is a training method in Machine Learning, which uses layers of neural nets to simulate how a human brain learns, and uses these neural net models to train the AI system to match inputs to correct outputs.

#### Foundational Models

Nowadays, systems employ neural networks, but
at a vastly different scale. (Supercomputers, thousands of GPUS, woven into a complex fabric of high-speed network interconnects and data-storage infrastructure.)

##### Pre-training

Neural network is first trained on a general purpose
dataset using significant amounts of computational resources, and then fine-tuned for the task at hand using a much smaller amount of data and compute resources.

e.g. GPT-3, DALL-E, CLIP, LAMDA

- quazel, make-a-video

